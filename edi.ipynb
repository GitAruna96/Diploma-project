{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ad108e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:40:56.587172Z",
     "iopub.status.busy": "2024-12-05T14:40:56.586835Z",
     "iopub.status.idle": "2024-12-05T14:45:49.674612Z",
     "shell.execute_reply": "2024-12-05T14:45:49.673427Z"
    },
    "papermill": {
     "duration": 293.09444,
     "end_time": "2024-12-05T14:45:49.676500",
     "exception": false,
     "start_time": "2024-12-05T14:40:56.582060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n",
      "Looking in links: /kaggle/input/bitsandbytes0-42-0\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.14.1)\r\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes==0.42.0) (1.26.4)\r\n",
      "Installing collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.42.0\r\n",
      "Looking in links: /kaggle/input/bitsandbytes0-42-0\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/coloredlogs-15.0.1-py2.py3-none-any.whl (from optimum==1.21.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (1.13.3)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/transformers-4.42.4-py3-none-any.whl (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (2.3.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (21.3)\r\n",
      "Requirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (0.25.1)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (3.0.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (2024.6.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum==1.21.2) (3.1.2)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (3.1.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (2.3.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum==1.21.2) (12.6.77)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2) (2024.5.15)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2) (0.4.5)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2) (3.20.3)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2) (0.2.0)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/humanfriendly-10.0-py2.py3-none-any.whl (from coloredlogs->optimum==1.21.2)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (3.9.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.21.2) (1.3.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.21.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.21.2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.21.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.21.2) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum==1.21.2) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.21.2) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.21.2) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.21.2) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.21.2) (1.16.0)\r\n",
      "Installing collected packages: humanfriendly, coloredlogs, tokenizers, transformers, optimum\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.20.0\r\n",
      "    Uninstalling tokenizers-0.20.0:\r\n",
      "      Successfully uninstalled tokenizers-0.20.0\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "logits-processor-zoo 0.1.0 requires accelerate<0.27.0,>=0.26.1, but you have accelerate 0.34.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.21.2 tokenizers-0.19.1 transformers-4.42.4\r\n",
      "Looking in links: /kaggle/input/bitsandbytes0-42-0\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (0.34.2)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (3.0.1)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (0.2.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (1.26.4)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/rouge-1.0.1-py3-none-any.whl (from auto-gptq==0.7.1)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/gekko-1.2.1-py3-none-any.whl (from auto-gptq==0.7.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (2.3.1)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (0.4.5)\r\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (4.42.4)\r\n",
      "Requirement already satisfied: peft>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (0.11.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (4.66.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq==0.7.1) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq==0.7.1) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq==0.7.1) (6.0.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq==0.7.1) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (2.3.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq==0.7.1) (12.6.77)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq==0.7.1) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq==0.7.1) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq==0.7.1) (0.19.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (3.9.5)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq==0.7.1) (1.16.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (4.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.26.0->auto-gptq==0.7.1) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq==0.7.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq==0.7.1) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq==0.7.1) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq==0.7.1) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq==0.7.1) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.7.1) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.7.1) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.7.1) (2024.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq==0.7.1) (1.3.0)\r\n",
      "Installing collected packages: rouge, gekko, auto-gptq\r\n",
      "Successfully installed auto-gptq-0.7.1 gekko-1.2.1 rouge-1.0.1\r\n",
      "CPU times: user 3.43 s, sys: 815 ms, total: 4.24 s\n",
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n",
    "!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\n",
    "!pip install transformers peft accelerate -q -U --no-index --find-links /kaggle/input/lmsys-wheel-files\n",
    "!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f78e38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:45:49.688898Z",
     "iopub.status.busy": "2024-12-05T14:45:49.688207Z",
     "iopub.status.idle": "2024-12-05T14:45:58.639716Z",
     "shell.execute_reply": "2024-12-05T14:45:58.638971Z"
    },
    "papermill": {
     "duration": 8.959686,
     "end_time": "2024-12-05T14:45:58.641721",
     "exception": false,
     "start_time": "2024-12-05T14:45:49.682035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic Python Libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# Data Processing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# PyTorch and Related\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "# Progress Bars\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/sfr-embedding-mistral/SFR-Embedding-2_R\"\n",
    "lora_path=\"/kaggle/input/v7-recall/epoch_19_model/adapter.bin\"\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8108ed9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:45:58.655116Z",
     "iopub.status.busy": "2024-12-05T14:45:58.654685Z",
     "iopub.status.idle": "2024-12-05T14:45:58.687132Z",
     "shell.execute_reply": "2024-12-05T14:45:58.686481Z"
    },
    "papermill": {
     "duration": 0.040124,
     "end_time": "2024-12-05T14:45:58.688743",
     "exception": false,
     "start_time": "2024-12-05T14:45:58.648619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "\n",
    "\n",
    "rows = []\n",
    "for idx, row in full_df.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row.CorrectAnswer:\n",
    "            continue\n",
    "            \n",
    "        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n",
    "\n",
    "        query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{correct_answer}\\n###Misconcepte Incorrect answer###:{option}.{row[f'Answer{option}Text']}\"\n",
    "\n",
    "        rows.append({\"query_text\": query_text, \n",
    "                     \"QuestionId_Answer\": f\"{row.QuestionId}_{option}\",\n",
    "                     \"ConstructName\": row.ConstructName,\n",
    "                     \"SubjectName\": row.SubjectName,\n",
    "                     \"QuestionText\": row.QuestionText,\n",
    "                     \"correct_answer\": correct_answer,\n",
    "                     \"incorrect_answer\": row[f\"Answer{option}Text\"]\n",
    "                     })\n",
    "\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156d5750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:45:58.699986Z",
     "iopub.status.busy": "2024-12-05T14:45:58.699737Z",
     "iopub.status.idle": "2024-12-05T14:45:58.708984Z",
     "shell.execute_reply": "2024-12-05T14:45:58.708357Z"
    },
    "papermill": {
     "duration": 0.01665,
     "end_time": "2024-12-05T14:45:58.710561",
     "exception": false,
     "start_time": "2024-12-05T14:45:58.693911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 16\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb2bcc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:45:58.721429Z",
     "iopub.status.busy": "2024-12-05T14:45:58.721187Z",
     "iopub.status.idle": "2024-12-05T14:47:23.905656Z",
     "shell.execute_reply": "2024-12-05T14:47:23.904854Z"
    },
    "papermill": {
     "duration": 85.192342,
     "end_time": "2024-12-05T14:47:23.907818",
     "exception": false,
     "start_time": "2024-12-05T14:45:58.715476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbc590067b6453aa809ee68528142ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(lora_path.replace(\"/adapter.bin\",\"\"))\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "backbone = AutoModel.from_pretrained(model_path, quantization_config=bnb_config,device_map=device)\n",
    "config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=256,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,  # Conventional\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "model = get_peft_model(backbone, config)\n",
    "d = torch.load(lora_path, map_location=model.device)\n",
    "model.load_state_dict(d, strict=False)\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3aea1b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:47:23.921422Z",
     "iopub.status.busy": "2024-12-05T14:47:23.920660Z",
     "iopub.status.idle": "2024-12-05T14:55:25.017254Z",
     "shell.execute_reply": "2024-12-05T14:55:25.016384Z"
    },
    "papermill": {
     "duration": 481.105901,
     "end_time": "2024-12-05T14:55:25.019549",
     "exception": false,
     "start_time": "2024-12-05T14:47:23.913648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6467777f7144df89779e6801118e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0385b0879b433abefa88b2964a5a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception. You need to thought carefully and reasonal'\n",
    "\n",
    "\n",
    "V_answer = inference(df, model, tokenizer, device)\n",
    "\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "misconception_df[\"query_text\"] = misconception_df[\"MisconceptionName\"]\n",
    "\n",
    "V_misconception = inference(misconception_df, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e42df53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:55:25.035268Z",
     "iopub.status.busy": "2024-12-05T14:55:25.034719Z",
     "iopub.status.idle": "2024-12-05T14:55:25.159576Z",
     "shell.execute_reply": "2024-12-05T14:55:25.158083Z"
    },
    "papermill": {
     "duration": 0.138062,
     "end_time": "2024-12-05T14:55:25.164309",
     "exception": false,
     "start_time": "2024-12-05T14:55:25.026247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 25)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def get_matches(V_topic, V_content, n_neighbors=25):\n",
    "    \n",
    "    neighbors_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm=\"brute\", n_jobs=-1)\n",
    "    neighbors_model.fit(V_content)\n",
    "    dists, indices = neighbors_model.kneighbors(V_topic)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "indices = get_matches(V_answer, V_misconception, n_neighbors=25)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4782465a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:55:25.214444Z",
     "iopub.status.busy": "2024-12-05T14:55:25.213724Z",
     "iopub.status.idle": "2024-12-05T14:55:26.022501Z",
     "shell.execute_reply": "2024-12-05T14:55:26.021431Z"
    },
    "papermill": {
     "duration": 0.832047,
     "end_time": "2024-12-05T14:55:26.024770",
     "exception": false,
     "start_time": "2024-12-05T14:55:25.192723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del backbone, model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "np.save(\"indices.npy\", indices)\n",
    "df.to_parquet(\"df.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088e5b76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:55:26.039770Z",
     "iopub.status.busy": "2024-12-05T14:55:26.038867Z",
     "iopub.status.idle": "2024-12-05T14:55:26.048102Z",
     "shell.execute_reply": "2024-12-05T14:55:26.047052Z"
    },
    "papermill": {
     "duration": 0.018804,
     "end_time": "2024-12-05T14:55:26.049969",
     "exception": false,
     "start_time": "2024-12-05T14:55:26.031165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import vllm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import re\n",
    "\n",
    "model_path = \"/kaggle/input/m/abdullahmeda/qwen2.5/transformers/math-7b-instruct/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "PROMPT  = \"\"\"Here is a question about {ConstructName}({SubjectName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n",
    "Answer concisely what misconception it is to lead to getting the incorrect answer.\n",
    "Pick the correct misconception number from the below:\n",
    "\n",
    "{Retrival}\n",
    "\"\"\"\n",
    "# just directly give your answers.\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"incorrect_answer\"],\n",
    "                    CorrectAnswer=row[f\"correct_answer\"],\n",
    "                    Retrival=row[f\"retrieval\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "df = pd.read_parquet(\"df.parquet\")\n",
    "indices = np.load(\"indices.npy\")\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    # quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=4096,\n",
    "    disable_log_stats=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "\n",
    "def get_candidates(c_indices):\n",
    "    candidates = []\n",
    "\n",
    "    mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "survivors = indices[:, -1:]\n",
    "\n",
    "for i in range(3):\n",
    "    c_indices = np.concatenate([indices[:, -8*(i+1)-1:-8*i-1], survivors], axis=1)\n",
    "    \n",
    "    df[\"retrieval\"] = get_candidates(c_indices)\n",
    "    df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "    \n",
    "    print(\"Example:\")\n",
    "    print(df[\"text\"].values[0])\n",
    "    print()\n",
    "    \n",
    "    responses = llm.generate(\n",
    "        df[\"text\"].values,\n",
    "        vllm.SamplingParams(\n",
    "            n=1,  # Number of output sequences to return for each prompt.\n",
    "            top_k=1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "            temperature=0,  # randomness of the sampling\n",
    "            seed=777, # Seed for reprodicibility\n",
    "            skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "            max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])]\n",
    "        ),\n",
    "        use_tqdm=True\n",
    "    )\n",
    "    \n",
    "    responses = [x.outputs[0].text for x in responses]\n",
    "    df[\"response\"] = responses\n",
    "    \n",
    "    \n",
    "    llm_choices = df[\"response\"].astype(int).values - 1\n",
    "    \n",
    "    survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(indices.shape[0]):\n",
    "    ix = indices[i]\n",
    "    llm_choice = survivors[i, 0]\n",
    "    \n",
    "    results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n",
    "\n",
    "\n",
    "df[\"MisconceptionId\"] = results\n",
    "df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)\n",
    "print('finish write file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe31661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:55:26.064443Z",
     "iopub.status.busy": "2024-12-05T14:55:26.064104Z",
     "iopub.status.idle": "2024-12-05T14:57:19.037622Z",
     "shell.execute_reply": "2024-12-05T14:57:19.036216Z"
    },
    "papermill": {
     "duration": 112.983423,
     "end_time": "2024-12-05T14:57:19.039986",
     "exception": false,
     "start_time": "2024-12-05T14:55:26.056563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "WARNING 12-05 14:55:32 config.py:1425] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-05 14:55:32 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 12-05 14:55:32 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/m/abdullahmeda/qwen2.5/transformers/math-7b-instruct/1', speculative_config=None, tokenizer='/kaggle/input/m/abdullahmeda/qwen2.5/transformers/math-7b-instruct/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/m/abdullahmeda/qwen2.5/transformers/math-7b-instruct/1, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "INFO 12-05 14:55:33 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 12-05 14:55:33 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-05 14:55:33 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:55:33 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:55:33 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:55:35 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-05 14:55:35 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:55:35 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:55:35 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-05 14:55:35 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-05 14:55:35 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-05 14:55:43 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:55:43 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 12-05 14:55:43 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m WARNING 12-05 14:55:43 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-05 14:55:43 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7964e05cf0d0>, local_subscribe_port=55529, local_sync_port=47353, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 12-05 14:55:44 model_runner.py:680] Starting to load model /kaggle/input/m/abdullahmeda/qwen2.5/transformers/math-7b-instruct/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:55:44 model_runner.py:680] Starting to load model /kaggle/input/m/abdullahmeda/qwen2.5/transformers/math-7b-instruct/1...\r\n",
      "INFO 12-05 14:55:44 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:55:44 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:55:44 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-05 14:55:44 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:19<00:59, 19.87s/it]\r\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:37<00:37, 18.64s/it]\r\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:53<00:17, 17.28s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:12<00:00, 17.94s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:12<00:00, 18.07s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=135)\u001b[0;0m INFO 12-05 14:56:56 model_runner.py:692] Loading model weights took 7.1145 GB\r\n",
      "INFO 12-05 14:56:56 model_runner.py:692] Loading model weights took 7.1145 GB\r\n",
      "INFO 12-05 14:56:59 distributed_gpu_executor.py:56] # GPU blocks: 9374, # CPU blocks: 9362\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "Please reason step by step, and put your final answer within \\boxed{}.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Does not reverse the order of operations when solving an equation\r\n",
      "2. Assumes that a list is given in size order\r\n",
      "3. Believes that the order of a worded calculation should be changed to follow BIDMAS \r\n",
      "4. Performs addition ahead of multiplication\r\n",
      "5. Believes the largest number in a set of numbers is always their highest common factor\r\n",
      "6. Answers order of operations questions with brackets as if the brackets are not there\r\n",
      "7. May have made a calculation error using the order of operations\r\n",
      "8. Believes the number of sides gives the order of rotational symmetry\r\n",
      "9. Believes the largest number in a set of numbers is always their lowest common multiple<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|| 9/9 [00:02<00:00,  4.19it/s, est. speed input: 1401.4\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "Please reason step by step, and put your final answer within \\boxed{}.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Performs addition ahead of any other operation\r\n",
      "2. Inserts brackets but not changed order of operation\r\n",
      "3. Thinks the order of events does not matter in a question about combined probability with no replacement\r\n",
      "4. Ordered from largest to smallest\r\n",
      "5. Confuses the order of operations, believes addition comes before division\r\n",
      "6. Believes addition comes before indices, in orders of operation\r\n",
      "7. Applies BIDMAS in strict order (does not realize addition and subtraction, and multiplication and division, are of equal priority)\r\n",
      "8. Does not interpret the correct order of operations in a fraction when there is an addition on the numerator\r\n",
      "9. Does not reverse the order of operations when solving an equation<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|| 9/9 [00:01<00:00,  6.71it/s, est. speed input: 2241.9\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "Please reason step by step, and put your final answer within \\boxed{}.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Confuses the order of operations, believes addition comes before multiplication \r\n",
      "2. Believes order of operations does not affect the answer to a calculation\r\n",
      "3. Carries out operations from right to left regardless of priority order\r\n",
      "4. Confuses the order of operations, believes subtraction comes before multiplication \r\n",
      "5. Performs subtraction in wrong order\r\n",
      "6. Does not follow the arrows through a function machine, changes the order of the operations asked.\r\n",
      "7. Carries out operations from left to right regardless of priority order\r\n",
      "8. Misunderstands order of operations in algebraic expressions\r\n",
      "9. Performs addition ahead of any other operation<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|| 9/9 [00:01<00:00,  5.50it/s, est. speed input: 1868.4\r\n",
      "finish write file\r\n",
      "INFO 12-05 14:57:14 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df9000b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:57:19.057755Z",
     "iopub.status.busy": "2024-12-05T14:57:19.057371Z",
     "iopub.status.idle": "2024-12-05T14:57:19.078420Z",
     "shell.execute_reply": "2024-12-05T14:57:19.077380Z"
    },
    "papermill": {
     "duration": 0.03208,
     "end_time": "2024-12-05T14:57:19.080269",
     "exception": false,
     "start_time": "2024-12-05T14:57:19.048189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1672 2532 706 1941 1054 77 1507 2586 1516 1345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>2306 2488 1941 655 1054 1689 117 375 1135 1372...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>1672 2532 158 77 1941 15 706 2586 1345 1392 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>2142 2068 143 2372 2078 418 320 2139 1904 1606...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>143 891 1871 363 1593 2549 413 1540 2078 979 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>2142 143 2068 418 2372 1872 1416 715 1319 1432...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>2408 1408 1923 1287 1073 397 2584 906 941 1498...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>2408 1408 1287 1923 1073 2584 2188 906 941 912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1073 1408 2408 906 1287 2439 2188 1923 2177 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  1672 2532 706 1941 1054 77 1507 2586 1516 1345...\n",
       "1            1869_C  2306 2488 1941 655 1054 1689 117 375 1135 1372...\n",
       "2            1869_D  1672 2532 158 77 1941 15 706 2586 1345 1392 11...\n",
       "3            1870_A  2142 2068 143 2372 2078 418 320 2139 1904 1606...\n",
       "4            1870_B  143 891 1871 363 1593 2549 413 1540 2078 979 1...\n",
       "5            1870_C  2142 143 2068 418 2372 1872 1416 715 1319 1432...\n",
       "6            1871_A  2408 1408 1923 1287 1073 397 2584 906 941 1498...\n",
       "7            1871_C  2408 1408 1287 1923 1073 2584 2188 906 941 912...\n",
       "8            1871_D  1073 1408 2408 906 1287 2439 2188 1923 2177 15..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5920031,
     "sourceId": 9688062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5957531,
     "sourceId": 9734430,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10100753,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 204642927,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 208327345,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 118363,
     "sourceId": 139763,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 174396,
     "modelInstanceId": 151944,
     "sourceId": 178348,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 989.64239,
   "end_time": "2024-12-05T14:57:22.378402",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-05T14:40:52.736012",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00b2e892218746ed83c9bf72f95b040c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_49f5529221df4c519a38f2a52fad6411",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dc3b23f299f1418a9580eada3b7acb94",
       "value": 1.0
      }
     },
     "10f580e606d549b4acdcc122ee6edaa5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "13bedf604d09450e9e031f0765823650": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21c9eb5e2d1a4d06934e457adb93938b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "337f5bfff5c14d079c541e8062c656e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "36fc9ccf7d8448ba986ad2ff7fbc8a43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_47d53a27d796453a890c1cb3a174d892",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ab61c162119641e8805a14705af8b49f",
       "value": 3.0
      }
     },
     "47d53a27d796453a890c1cb3a174d892": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "486a7a805bf542118ff338d2eb464a5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fcbf81544be24e09a1e7523da0f01e0e",
       "max": 162.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f5be2c7c66c44a7c8bd456db178bf133",
       "value": 162.0
      }
     },
     "49f5529221df4c519a38f2a52fad6411": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "51e884e314584f9a84cb151e76e9476a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "55c3790fdc16489c99c9a165839e5651": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5ee79ff312264126b950fe2192c0617b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_51e884e314584f9a84cb151e76e9476a",
       "placeholder": "",
       "style": "IPY_MODEL_b798aa9a31ac4701bd66f1ca0cc47fa5",
       "value": "Loadingcheckpointshards:100%"
      }
     },
     "7012c6ad60534fa8bf2df2f68b42bda1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73e675eb3af94d65b3f63f39c8fb1ffe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77817a37815741ff960c7b738a424640": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7875e58b84af4691a286e060339fb10f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a5dc3863f2fc4dd28c1ad595e6e2f5b1",
       "placeholder": "",
       "style": "IPY_MODEL_55c3790fdc16489c99c9a165839e5651",
       "value": "3/3[01:20&lt;00:00,26.42s/it]"
      }
     },
     "7ed5cf30d3c147a79c61e216a845b1cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_73e675eb3af94d65b3f63f39c8fb1ffe",
       "placeholder": "",
       "style": "IPY_MODEL_10f580e606d549b4acdcc122ee6edaa5",
       "value": "Batches:100%"
      }
     },
     "8fef767922e44db6b019415b65cf32a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9118d1de537e4a799dff612e1bf7d4a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a0f4e34a1efb41bc894d604901f7c204": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9118d1de537e4a799dff612e1bf7d4a9",
       "placeholder": "",
       "style": "IPY_MODEL_337f5bfff5c14d079c541e8062c656e0",
       "value": "1/1[00:25&lt;00:00,25.51s/it]"
      }
     },
     "a5dc3863f2fc4dd28c1ad595e6e2f5b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ab61c162119641e8805a14705af8b49f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b798aa9a31ac4701bd66f1ca0cc47fa5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c07e9ca89e3b4c5494883888d09a9a46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_13bedf604d09450e9e031f0765823650",
       "placeholder": "",
       "style": "IPY_MODEL_21c9eb5e2d1a4d06934e457adb93938b",
       "value": "Batches:100%"
      }
     },
     "c1e51afd752c4ff0894cad0d85f004db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ccbc590067b6453aa809ee68528142ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5ee79ff312264126b950fe2192c0617b",
        "IPY_MODEL_36fc9ccf7d8448ba986ad2ff7fbc8a43",
        "IPY_MODEL_7875e58b84af4691a286e060339fb10f"
       ],
       "layout": "IPY_MODEL_77817a37815741ff960c7b738a424640"
      }
     },
     "db3781e75eba42acab287f66af8ca993": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc0385b0879b433abefa88b2964a5a7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c07e9ca89e3b4c5494883888d09a9a46",
        "IPY_MODEL_486a7a805bf542118ff338d2eb464a5c",
        "IPY_MODEL_e799f4e8ac0f462d8ce92fba3abd579e"
       ],
       "layout": "IPY_MODEL_8fef767922e44db6b019415b65cf32a1"
      }
     },
     "dc3b23f299f1418a9580eada3b7acb94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e799f4e8ac0f462d8ce92fba3abd579e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7012c6ad60534fa8bf2df2f68b42bda1",
       "placeholder": "",
       "style": "IPY_MODEL_c1e51afd752c4ff0894cad0d85f004db",
       "value": "162/162[07:34&lt;00:00,1.94s/it]"
      }
     },
     "f5be2c7c66c44a7c8bd456db178bf133": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fc6467777f7144df89779e6801118e51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7ed5cf30d3c147a79c61e216a845b1cd",
        "IPY_MODEL_00b2e892218746ed83c9bf72f95b040c",
        "IPY_MODEL_a0f4e34a1efb41bc894d604901f7c204"
       ],
       "layout": "IPY_MODEL_db3781e75eba42acab287f66af8ca993"
      }
     },
     "fcbf81544be24e09a1e7523da0f01e0e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
